{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lambdal\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "C:\\Users\\lambdal\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import jieba\n",
    "import yaml\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "from sklearn.cross_validation import train_test_split  \n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import model_from_yaml\n",
    "from keras.optimizers import SGD, RMSprop, Adagrad\n",
    "# from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "\n",
    "from __future__ import absolute_import #导入3.x的特征函数\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 加载训练文件并分词\n",
    "#加载训练文件\n",
    "def loadfile():\n",
    "    neg=pd.read_excel('./data/neg.xls',header=None,index=None)\n",
    "    pos=pd.read_excel('./data/pos.xls',header=None,index=None)\n",
    "\n",
    "    pos['mark']=1\n",
    "    neg['mark']=0 #给训练语料贴上标签\n",
    "    pn=pd.concat([pos,neg],ignore_index=True) #合并语料\n",
    "    neglen=len(neg)\n",
    "    poslen=len(pos) #计算语料数目\n",
    "    \n",
    "    cw = lambda x: list(jieba.cut(x)) #定义分词函数\n",
    "    pn['words'] = pn[0].apply(cw)\n",
    "    \n",
    "    comment = pd.read_excel('./data/sum.xls') #读入评论内容\n",
    "    comment = comment[comment['rateContent'].notnull()] #仅读取非空评论\n",
    "    comment['words'] = comment['rateContent'].apply(cw) #评论分词 \n",
    "\n",
    "    d2v_train = pd.concat([pn['words'], comment['words']], ignore_index = True) \n",
    "    \n",
    "    \n",
    "    return pn['words'],pn['mark']\n",
    "#     return d2v_train\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 创建词语字典，并返回每个词语的索引，词向量，以及每个句子所对应的词语索引\n",
    "def create_dictionaries(model=None, combined=None):\n",
    "    ''' Function does are number of Jobs:\n",
    "        1- Creates a word to index mapping\n",
    "        2- Creates a word to vector mapping\n",
    "        3- Transforms the Training and Testing Dictionaries\n",
    "\n",
    "    '''\n",
    "    if (combined is not None) and (model is not None):\n",
    "        gensim_dict = Dictionary()\n",
    "        gensim_dict.doc2bow(model.wv.vocab.keys(), allow_update=True)\n",
    "        w2indx = {v: k+1 for k, v in gensim_dict.items()}#所有频数超过10的词语的索引\n",
    "        w2vec = {word: model[word] for word in w2indx.keys()}#所有频数超过10的词语的词向量\n",
    "\n",
    "        def parse_dataset(combined):\n",
    "            ''' Words become integers\n",
    "            '''\n",
    "            data=[]\n",
    "            for sentence in combined:\n",
    "                new_txt = []\n",
    "                for word in sentence:\n",
    "                    try:\n",
    "                        new_txt.append(w2indx[word])\n",
    "                    except:\n",
    "                        new_txt.append(0)\n",
    "                data.append(new_txt)\n",
    "            return data\n",
    "        \n",
    "        combined = parse_dataset(combined)\n",
    "        maxlen = 64\n",
    "        combined = sequence.pad_sequences(combined, maxlen=maxlen)#填充序列，对于长于maxlen的序列进行截断，短语maxlen的序列在后面进行填充0\n",
    "        return w2indx, w2vec,combined\n",
    "    else:\n",
    "        print('No data provided...')\n",
    "\n",
    "\n",
    "#创建词语字典，并返回每个词语的索引，词向量，以及每个句子所对应的词语索引\n",
    "def word2vec_train(combined):\n",
    "\n",
    "    vocab_dim = 300\n",
    "    n_exposures = 10\n",
    "    \n",
    "    model = Word2Vec(sentences=combined, size=vocab_dim, min_count=n_exposures)\n",
    "#     model.build_vocab(combined)\n",
    "#     model.train(combined)\n",
    "    model.save('./lstm_data/Word2vec_model.pkl')\n",
    "    index_dict, word_vectors,combined = create_dictionaries(model=model,combined=combined)\n",
    "    return   index_dict, word_vectors,combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 训练网络，并保存模型，其中LSTM的实现采用Python中的keras库\n",
    "\n",
    "def get_data(index_dict,word_vectors,combined,y):\n",
    "    vocab_dim = 300\n",
    "    n_symbols = len(index_dict) + 1  # 所有单词的索引数，频数小于10的词语索引为0，所以加1\n",
    "    embedding_weights = np.zeros((n_symbols, vocab_dim))#索引为0的词语，词向量全为0\n",
    "    for word, index in index_dict.items():#从索引为1的词语开始，对每个词语对应其词向量\n",
    "        embedding_weights[index, :] = word_vectors[word]\n",
    "    x_train, x_test, y_train, y_test = train_test_split(combined, y, test_size=0.2)\n",
    "\n",
    "    return n_symbols,embedding_weights,x_train,y_train,x_test,y_test\n",
    "\n",
    "\n",
    "##定义网络结构\n",
    "def train_lstm(n_symbols,embedding_weights,x_train,y_train,x_test,y_test):\n",
    "    \n",
    "    print('Defining a Simple Keras Model...')\n",
    "    \n",
    "    model = Sequential()  # or Graph or whatever\n",
    "    model.add(Embedding(input_dim=n_symbols,\n",
    "                        output_dim=300,    \n",
    "                        mask_zero=True,\n",
    "                        weights=[embedding_weights],\n",
    "                        input_length=64))  # 当输入序列的长度固定时，该值为其长度,输入序列维度。\n",
    "    model.add(LSTM(128, input_dim=300, activation='sigmoid', inner_activation='hard_sigmoid'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "\n",
    "    print('Compiling the Model...')\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "    print(\"Train...\")\n",
    "    model.fit(x_train, y_train, batch_size=16, nb_epoch=10,verbose=1, validation_data=(x_test, y_test))\n",
    "\n",
    "    print(\"Evaluate...\")\n",
    "    score = model.evaluate(x_test, y_test,\n",
    "                                batch_size=16)\n",
    "\n",
    "    yaml_string = model.to_yaml()\n",
    "    with open('./lstm_data/lstm.yml', 'w') as outfile:\n",
    "        outfile.write( yaml.dump(yaml_string, default_flow_style=True) )\n",
    "    model.save_weights('./lstm_data/lstm.h5')\n",
    "    print('Test score:', score)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#训练模型，并保存\n",
    "def train():\n",
    "    print('Loading Data...')\n",
    "    comment, y =loadfile()\n",
    "    comment = comment[5000:15000]\n",
    "    y = y[5000:15000]\n",
    "    \n",
    "    print(len(comment),len(y))\n",
    "    # print('Tokenising...')\n",
    "    # comment = tokenizer(comment)\n",
    "    print('Training a Word2vec model...')\n",
    "    index_dict, word_vectors,comment = word2vec_train(comment)\n",
    "    \n",
    "    print('Setting up Arrays for Keras Embedding Layer...')\n",
    "    n_symbols,embedding_weights,x_train,y_train,x_test,y_test=get_data(index_dict, word_vectors,comment,y)\n",
    "    print(x_train.shape,y_train.shape)\n",
    "    \n",
    "    train_lstm(n_symbols,embedding_weights,x_train,y_train,x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(model, test):\n",
    "    label = model.predict(test)\n",
    "    \n",
    "    s = []\n",
    "    for item in label:\n",
    "        if item>=0.5:\n",
    "            s.append(1)\n",
    "        else :\n",
    "            s.append(0)\n",
    "    \n",
    "    import matplotlib\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.hist(s, bins=np.arange(0,1.2,0.25))\n",
    "    plt.show()\n",
    "\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    train()\n",
    "    \n",
    "    with open('./lstm_data/lstm.yml', 'r') as outfile:\n",
    "         #导入  \n",
    "        yaml_string = yaml.load(outfile) \n",
    "    \n",
    "\n",
    "    model = model_from_yaml(yaml_string)\n",
    "    model.load_weights('./lstm_data/lstm.h5', by_name=False)\n",
    "\n",
    "    cw = lambda x: list(jieba.cut(x)) #定义分词函数\n",
    "\n",
    "    comment = pd.read_excel('./data/sum.xls') #读入评论内容\n",
    "    comment = comment[comment['rateContent'].notnull()] #仅读取非空评论\n",
    "    comment['words'] = comment['rateContent'].apply(cw) #评论分词 \n",
    "    data = comment['words']\n",
    "\n",
    "    index_dict, word_vectors,comment = word2vec_train(data)\n",
    "    label = predict(model, comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\lambdal\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 1.060 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 10000\n",
      "Training a Word2vec model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lambdal\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:13: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up Arrays for Keras Embedding Layer...\n",
      "(8000, 64) (8000,)\n",
      "Defining a Simple Keras Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lambdal\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:25: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "C:\\Users\\lambdal\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:25: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(128, activation=\"sigmoid\", input_shape=(None, 300..., recurrent_activation=\"hard_sigmoid\")`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling the Model...\n",
      "Train...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lambdal\\Anaconda3\\lib\\site-packages\\keras\\models.py:848: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/10\n",
      "8000/8000 [==============================] - 85s - loss: 0.3320 - acc: 0.8483 - val_loss: 0.1204 - val_acc: 0.9600\n",
      "Epoch 2/10\n",
      "8000/8000 [==============================] - 85s - loss: 0.0812 - acc: 0.9726 - val_loss: 0.1020 - val_acc: 0.9600\n",
      "Epoch 3/10\n",
      "8000/8000 [==============================] - 85s - loss: 0.0382 - acc: 0.9881 - val_loss: 0.2126 - val_acc: 0.9410\n",
      "Epoch 4/10\n",
      "8000/8000 [==============================] - 155s - loss: 0.0274 - acc: 0.9912 - val_loss: 0.0977 - val_acc: 0.9700\n",
      "Epoch 5/10\n",
      "8000/8000 [==============================] - 85s - loss: 0.0134 - acc: 0.9959 - val_loss: 0.1213 - val_acc: 0.9700\n",
      "Epoch 6/10\n",
      "8000/8000 [==============================] - 165s - loss: 0.0074 - acc: 0.9984 - val_loss: 0.1379 - val_acc: 0.9715\n",
      "Epoch 7/10\n",
      "8000/8000 [==============================] - 86s - loss: 0.0089 - acc: 0.9979 - val_loss: 0.1204 - val_acc: 0.9720\n",
      "Epoch 8/10\n",
      "8000/8000 [==============================] - 172s - loss: 0.0083 - acc: 0.9981 - val_loss: 0.3742 - val_acc: 0.9370\n",
      "Epoch 9/10\n",
      "8000/8000 [==============================] - 99s - loss: 0.0120 - acc: 0.9969 - val_loss: 0.1449 - val_acc: 0.9735\n",
      "Epoch 10/10\n",
      "8000/8000 [==============================] - 163s - loss: 0.0062 - acc: 0.9982 - val_loss: 0.1720 - val_acc: 0.9715\n",
      "Evaluate...\n",
      "2000/2000 [==============================] - 6s     \n",
      "Test score: [0.1720459833805362, 0.97150000000000003]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAEhSRxkAktRRBoAkddT/B0VGxQOHm2ALAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x17cb68f4128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 20min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 字典长度为3845\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
